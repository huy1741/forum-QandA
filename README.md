# Data Processing DAG with Spark and Airflow

## Introduction

This repository contains the code and configurations for a Data Processing DAG (Directed Acyclic Graph) using Apache Airflow and Apache Spark. The DAG is designed to process data from various sources, including CSV files, and store the results in MongoDB. It automates the entire data processing pipeline, making it easier to handle data extraction, transformation, and loading (ETL) tasks.

## Installation

Create a new folder and copy all the content to that folder. Open the terminal while in that folder and run docker-compose up -d to install. Then copy the dag file to folder dags and the spark file to the spark folder. While in Airflow, add spark connection with conn_id: spark_default with port 8181.

## DAG Structure

The DAG consists of several tasks, each with specific responsibilities. Here's an overview of the DAG's structure and tasks:

### Task 1: Start and End Dummy Operators

- **start_task**: A dummy task indicating the start of the DAG.
- **end_task**: A dummy task indicating the end of the DAG.

### Task 2: Branching Task

- **branching_task**: A PythonOperator that checks if the required data files (Questions.csv and Answers.csv) exist. If the files exist, the DAG proceeds to the end; otherwise, it goes to the "clear_files" task to remove any existing files.

### Task 3: Clear Files

- **clear_files_task**: A BashOperator that removes existing data files (Questions.csv and Answers.csv) to ensure a clean slate for downloading new data.

### Task 4: Download Question File

- **download_question_file_task**: A PythonOperator that uses the `GoogleDriveDownloader` class to download the Questions.csv file from a Google Drive link.

### Task 5: Download Answer File

- **download_answer_file_task**: A PythonOperator that uses the `GoogleDriveDownloader` class to download the Answers.csv file from a Google Drive link.

### Task 6: Import Questions into MongoDB

- **import_questions_mongo**: A BashOperator that uses the `mongoimport` command to import the Questions.csv data into MongoDB. It specifies the database, collection, and file path.

### Task 7: Import Answers into MongoDB

- **import_answers_mongo**: A BashOperator that uses the `mongoimport` command to import the Answers.csv data into MongoDB. It specifies the database, collection, and file path.

### Task 8: Spark Process

- **spark_process**: A SparkSubmitOperator that submits a Spark job for data processing. It uses a Spark script located at `/usr/local/share/spark/spark_script.py`. The Spark job calculates the number of answers for each question and writes the results to a CSV file.

### Task 9: Import Output into MongoDB

- **import_output_mongo**: A BashOperator that uses the `mongoimport` command to import the output CSV file generated by the Spark job into MongoDB.

## DAG Execution Flow

1. The DAG starts with the `start_task` and checks if the required data files exist (`branching_task`).
2. If the data files exist, the DAG proceeds to the `end_task`, indicating that no further action is needed.
3. If the data files do not exist, the DAG removes any existing files (`clear_files_task`).
4. After clearing existing files, the DAG downloads the Questions.csv and Answers.csv files from Google Drive (`download_question_file_task` and `download_answer_file_task`).
5. Once the data files are downloaded, the DAG imports the data into MongoDB collections (`import_questions_mongo` and `import_answers_mongo`).
6. Next, the DAG submits a Spark job (`spark_process`) to process the data. The Spark job calculates the number of answers for each question and writes the results to a CSV file.
7. Finally, the DAG imports the Spark job's output CSV file into MongoDB (
